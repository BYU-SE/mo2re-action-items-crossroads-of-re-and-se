#
# 13.yaml | Honeycomb
#
# Gradual failure during 4 minutes then complete outage for 8 minutes. Root cause was bad code that was deployed (a regression where a system lost a fature, committed code that didn't compile passed review and made it to master branch, build artifacts were missing and got deployed anyway)
# 
# https://www.honeycomb.io/blog/incident-review-you-cant-deploy-binaries-that-dont-exist


13: 

  #  As part of that work, refactoring some code led to a regression where the buildevents tool lost a feature - passing through the exit code of the commands that it runs [...] The build artifact from the non-compiling code was missing one executable, and got deployed anyway [...] As we'd spent a large portion of our error budget during this outage, we first instituted a release freeze on the API server. Then, we conducted a retrospective to identify what we steps we'd need to take in order to unfreeze. As of today, we have added safeguards in several places to mitigate the chances that any of the steps above could cause a similar outage.

  13-1:
    event: Non-compiling code deployed to production due to CI defect
    Event: Defect deployed (missed in pipeline)

    action: Institute release freeze 
    Action: Halt releases

    # TODO: JS: should this be the API server
    target: API server release pipeline (automated deployment system) 
    Target: Tech / Infrastructure / Automated deployment system

    goal: Prevent repeat incident until confidence restored
    Goal: Prevent reoccurrence (temporary) 

    memos:
      # "we first instituted a release freeze [..] Then, we conducted a retrospective [..] As of today, we have added safeguards in several places "
      - First-Fast-Then-Right: (Fast) Release freeze to allow retrospective then safeguards (unspecified).
    
      # As we'd spent a large portion of our error budget during this outage,
      - Error budget - time or money or availability quota?
      - This is kind of a META action (do nothing until after retrospective)

      # ... mitigate the chances that any of the steps above could cause a similar outage.

      - Address a problem-class at the infrastructure layer (beyond fixing defects)

      # safeguards in several places to mitigate the chances that any of the steps above could cause a similar outage.
      - Defense-In-Depth: (13-1, 13-2, 13-3, 13-5) Similar outage safeguards in several places (halt to plan the defense)

      - Similar: Halting to prevent "similar outage" from occurring in the near future (temporary)
  
  13-2:
    event: Non-compiling code deployed to production due to CI defect
    Event: Defect deployed (missed in pipeline)

    action: Make plans to unfreeze
    Action: Plan / Retrospective to allow unfreeze

    # TODO: JS: should this be the API server
    target: API server release pipeline (automated deployment system) 
    Target: Tech / Infrastructure / Automated deployment system

    goal: Prevent repeat incident until confidence restored
    Goal: Prevent reoccurrence (temporary) 

    memos:
      # "we first instituted a release freeze [..] Then, we conducted a retrospective [..] As of today, we have added safeguards in several places "
      - First-Fast-Then-Right: (Right) Retrospective to discover the correct safeguards (unspecified).

      # As we'd spent a large portion of our error budget during this outage,
      - Error budget - time or money or availability quota?
      - This is kind of a META action (do nothing until after retrospective)

      # ... mitigate the chances that any of the steps above could cause a similar outage.
      - Address a problem-class at the infrastructure layer (beyond fixing defects)

      # safeguards in several places to mitigate the chances that any of the steps above could cause a similar outage.
      - Defense-In-Depth: (13-1, 13-2, 13-3, 13-5) Similar outage safeguards in several places (planning for defense in depth)

      - Similar: Halting to prevent similar outage from occurring in the near future (temporary)

  # Much of the speed at which we iterate and deploy code depends on trusting that our build system is protecting us from errors like this. It’s paramount that we retain this trust in order to be able to use automated deploy systems, so the first step was to fix the regression in our build instrumentation tool and work towards rebuilding the belief that if the build is green it’s okay to deploy.
    
  13-3:
    event: Non-compiling code deployed to production due to CI defect
    Event: Defect deployed (missed in pipeline)

    action: Fix recent regression (fix exit-code handling for builds)
    Action: Fix defect / Fix error code handing

    target: Build portion of continuous integration pipeline 
    Target: Tech / Infrastructure / Automated deployment system

    goal: Prevent deployment of non-compiling code (and build trust in "green")
    Goal: Safe deployment / Prevent deployment of non-compiling code

    memos:
      # safeguards in several places to mitigate the chances that any of the steps above could cause a similar outage.
      - Defense-In-Depth: (13-1, 13-2, 13-3, 13-5) Similar outage safeguards in several places (build portion)

  # We can add strength to the belief in our automated build system with real world use - we can both trust that the builds are reliable, and verify that it is indeed the case by health-checking tasks with automatic rollback, and staggering deploys by letting production lag our internal dogfooding cluster by a build. This is common practice and has long been on the list of things to do but not yet prioritized. Now it has happened.

  13-4:
    event: Health checks noticed errors but just removed servers from fleet
    Event: Unhelpful health-check actions taken

    action: Add health-checking tasks with automatic rollback
    Action: Add / Feature / Health-checking and automatic rollback 

    target: Production health-checking tasks and actions
    Target: Tech / Component / Monitoring systems / Health-checking actions

    goal: Rollback bad builds that make it to production (trust and verify)
    Goal: Catch and rollback defective builds # JS: this could be a "reduce time to ..." sort of goal, I think?

    memos:
      # This is common practice and has long been on the list of things to do but not yet prioritized. Now it has happened.
      - Prioritization: Do something that is known to be good practice but on the backlog

  13-5:
    event: Non-compiling code deployed to production due to CI defect
    Event: Defect deployed (missed in pipeline)

    action: Add staggering CI pipeline (use build internally before production)
    Action: Add / Feature / Staggering of builds (between pre-production and production)

    target: Configuration of automated deployment system stages
    Target: Tech / Infrastructure / Automated deployment system / Staging configuration

    goal: Use builds internally before moving to production (build trust)
    Goal: Ensure builds are reliable / Use builds internally before production

    memos:
      # This is common practice and has long been on the list of things to do but not yet prioritized. Now it has happened.
      - Prioritization: Do something that is known to be good practice but on the backlog

      # safeguards in several places to mitigate the chances that any of the steps above could cause a similar outage.
      - Defense-In-Depth: (13-1, 13-2, 13-3, 13-5) Similar outage safeguards in several places (staging config)


  patterns:

    # we first instituted a release freeze on the API server. Then, we conducted a retrospective to identify what we steps we'd need to take in order to unfreeze. As of today, we have added safeguards in several places to mitigate the chances that any of the steps above could cause a similar outage.

    - 
      Freeze-plan-fix-unfreeze: Halt action (deployment) to gain confidence that a repeat incident during the action (deployment) won't occur
      First-Fast-Then-Right: Rhymes with this pattern. (Fast) Freeze and then (Right) do RCA and add PAs to fix
      Prioritization: Rhymes with this pattern. Halt work to not cause further issues. Concept of "safety over innovation or development"
      

    # As with many outages, it was a combination of factors that came together to trigger the incident [regression in pipeline, straight to prod and dogfood at the same time, unhelpful health check actions] 
    - Complex-Factors: Fixing multiple factors (though in a narror sense one would be enough) 1. Fix pipeline 2. Fix deployment staging 3. Fix health-check actions

    # "we'd spent a large portion of our error budget during this outage [..]"
    - 
      Error-budget: Allowable downtime
      Always-Present-Failure: Rhymes with this pattern.

  memos:

    # Much of the speed at which we iterate and deploy code depends on trusting that our build system is protecting us from errors like this. It’s paramount that we retain this trust in order to be able to use automated deploy systems, so the first step was to fix the regression in our build instrumentation tool and work towards rebuilding the belief that if the build is green it’s okay to deploy. [JS: This is super interesting IMO]

    - 
      Guiding-Philosophy: Philosopy that guided the choice of PAs (often implicit in reports)
      Restoring-Trust: Restoring trust is abstract/high-level goal for most PAs in this report